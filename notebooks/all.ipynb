{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STARTING_DOCUMENT_INDEX = 54711\n",
    "TOTAL_NUMBER_OF_DOCUMENTS = 293856\n",
    "\n",
    "CORPUS_PATH = '../data/ohsumed.88-91'\n",
    "QUERY_PATH = '../data/query.ohsu.1-63'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter \n",
    "import numpy as np \n",
    "\n",
    "def collect_document_ids(corpus):\n",
    "    '''collect a single list of document_ids given the database file'''\n",
    "    document_ids = []\n",
    "    for document in corpus: \n",
    "\n",
    "        pattern = \"\\.U\\n(.*?)\\n\"\n",
    "        doc_id_find = re.search(pattern, document)\n",
    "        if doc_id_find:\n",
    "            doc_id = doc_id_find.group(1)\n",
    "            document_ids.append(doc_id)\n",
    "    return document_ids\n",
    "\n",
    "\n",
    "def split_into_documents(database_file):\n",
    "    '''Splits the singular corpus into an array of documents'''\n",
    "    database_file = open(database_file)\n",
    "    data = database_file.read()\n",
    "    pattern = re.compile(\"\\.I\\s\\d*\\n\")\n",
    "    corpus = re.split(pattern, data)[1:]  # The first item is ''\n",
    "    return corpus\n",
    "\n",
    "\n",
    "\n",
    "def clean_entry(entry):\n",
    "    '''Clean a single document by removing headers, punctuation, tabs, spaces, newlines, and digits'''\n",
    "    # Remove . Headers\n",
    "    entry = re.sub('\\.[A-Z]', '', entry)\n",
    "\n",
    "    # Remove punctuation\n",
    "    entry = re.sub(r'[^\\w\\s]', ' ', entry)\n",
    "\n",
    "    # Remove \\n, \\r and \\t\n",
    "    entry = re.sub(r'[\\n\\t\\r]', ' ', entry)\n",
    "\n",
    "    # Remove digits\n",
    "    entry = re.sub(r'[\\d]', '', entry)\n",
    "\n",
    "    # Remove double spaces\n",
    "    entry = re.sub(' +', ' ', entry)\n",
    "\n",
    "    # Remove isolated small case letters\n",
    "    entry = re.sub(r\"\\b\\d+\\b *|\\b[a-z]\\b *\",\"\",entry)\n",
    "\n",
    "    # Remove leading and trailing spaces\n",
    "    entry = entry.strip()\n",
    "\n",
    "    return entry\n",
    "\n",
    "def process_query(query):\n",
    "    patterns = ['<num>.*\\n', '<desc>.*\\n', '<.*>']\n",
    "    query_id_find = re.search('<num>\\sNumber:\\s(.+?)\\n', query)\n",
    "    if query_id_find:\n",
    "        query_id = query_id_find.group(1)\n",
    "    for pattern in patterns: \n",
    "        query = re.sub(pattern, ' ', query)\n",
    "\n",
    "    # Split query to remove stopwords first\n",
    "    query = query.split(' ')\n",
    "    query = [q.lower() for q in query]\n",
    "    stop_words_list = read_stopwords_file('stop_words_english.txt')\n",
    "    query = remove_stopwords(query, stop_words_list)\n",
    "\n",
    "    # join the query again for the cleaning process\n",
    "    query = ' '.join(query)\n",
    "    query = clean_entry(query)\n",
    "\n",
    "    # Split query into tokens \n",
    "    query = query.split(' ')\n",
    "    \n",
    "    \n",
    "    return query, query_id\n",
    "\n",
    "def read_stopwords_file(filename):\n",
    "    '''Read stopwords from an external file'''\n",
    "    with open('stopwords.txt') as f:\n",
    "        stopwords_string = f.read()\n",
    "        stop_words_list = stopwords_string.split('\\n')\n",
    "    return stop_words_list\n",
    "\n",
    "def remove_stopwords(tokens, stop_words_list):\n",
    "    '''Remove all stopwords from a list of tokens using a stop_words_list'''\n",
    "    return [tok for tok in tokens if tok not in stop_words_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# from src.data_processing import clean_entry, read_stopwords_file, remove_stopwords\n",
    "import numpy as np\n",
    "import json \n",
    "\n",
    "\n",
    "\n",
    "def create_inverted_index(corpus, doc_ids):\n",
    "    '''Created inverted index for all unique terms with (document_ids, term_frequncies) in posting list'''\n",
    "    # Create empty inverted index \n",
    "    inverted_index = {}\n",
    "    document_lengths = np.zeros(len(corpus))\n",
    "\n",
    "    # Read stopwords from a stopword file \n",
    "    stopwords_list = read_stopwords_file('stop_words_english.txt')\n",
    "\n",
    "    # Iterate over the entire courpus of list of documents \n",
    "    for i, document in enumerate(corpus):\n",
    "        doc_id = i\n",
    "\n",
    "\n",
    "        document = document.split(\" \")\n",
    "\n",
    "        # Remove stopwords from document \n",
    "        document = remove_stopwords(document, stopwords_list)\n",
    "        document = [token.lower() for token in document]\n",
    "        \n",
    "        document = ' '.join(document)\n",
    "    \n",
    "        # Clean document \n",
    "        document = clean_entry(document)\n",
    "\n",
    "        # Tokenize document \n",
    "        tokens = document.split(\" \")\n",
    "\n",
    "        # Calculate and store document lengths \n",
    "        document_lengths[doc_id] = len(tokens)\n",
    "\n",
    "        # Create a dictionary which stores term frequencies \n",
    "        term_frequencies = Counter(tokens)\n",
    "\n",
    "        # Euclidian normalize the term frequencies \n",
    "        # denom = np.sum(np.array([(count)**2 for count in term_frequencies.values()]))\n",
    "        # denom = np.sqrt(denom)            \n",
    "        # Iterate over all unique terms \n",
    "        for term in term_frequencies.keys():\n",
    "            # term_frequencies[term] /= deno\n",
    "            # If the term already exists in inverted index, append the current (doc_id, term_frequency) to the postings list                \n",
    "            if term in inverted_index:\n",
    "                inverted_index[term].append((doc_id, term_frequencies[term]))\n",
    "            # If the term does not exist in inverted index, create a entry with the term as key nd add (doc_id, term_frequency) to the posting list\n",
    "            else:\n",
    "                inverted_index[term] = [(doc_id, term_frequencies[term])]\n",
    "    return inverted_index, document_lengths\n",
    "\n",
    "def get_document_norm(corpus):\n",
    "    norms = np.zeros(len(corpus))\n",
    "    for i, document in enumerate(corpus): \n",
    "        term_frequencies = Counter(document)\n",
    "        norm = np.sum(np.array([(count)**2 for count in term_frequencies.values()]))\n",
    "        norm = np.sqrt(norm) \n",
    "        norms[i] = norm \n",
    "    return norms  \n",
    "\n",
    "def calculate_idf(inverted_index, num_documents):\n",
    "    '''Calculate the inverse document frequency for all unique terms in the vocabulary'''\n",
    "    # Create empty idf dictionary \n",
    "    idf = {}\n",
    "\n",
    "    # Iterate over all terms in inverted index \n",
    "    for item in inverted_index.keys():\n",
    "        # IDF_t = log( N / number_of_documents_t_appears_in)\n",
    "        idf[item] = np.log(num_documents/(len(inverted_index[item])))\n",
    "    \n",
    "    return idf \n",
    "\n",
    "def save_inverted_index(inverted_index):\n",
    "    '''Save the inverted index to a json file'''\n",
    "    with open('inverted_index.json', 'w') as j:\n",
    "        json.dump(inverted_index, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_documents(rel_file):\n",
    "    \n",
    "    rel_docs = []\n",
    "    with open(rel_file) as f:\n",
    "        rel_file_string = f.read()\n",
    "    for i in range(63):\n",
    "        all_current_docs = re.search('OHSU1\\t0(.*?)\\t[0-9]', rel_file_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_file_string = get_relevant_documents('../data/qrels.ohsu.88-91')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    norm = 1.\n",
    "    alpha = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "# from src.config import TOTAL_NUMBER_OF_DOCUMENTS, STARTING_DOCUMENT_INDEX\n",
    "# from src.data_processing import process_query, remove_stopwords, read_stopwords_file, clean_entry\n",
    "from collections import Counter \n",
    "\n",
    "\n",
    "def compute_tf_idf_scores(inverted_index, query, idf, doc_ids, norms, Lengths):\n",
    "    scores = np.zeros(len(doc_ids))\n",
    "    tf_query = Counter(query)\n",
    "    for t in tf_query.keys():\n",
    "        if t in inverted_index:\n",
    "            w_tq = ( tf_query[t] ) * (idf[t] ** 2 )\n",
    "            postings_list = inverted_index[t]\n",
    "            for (d, tf_td) in postings_list:\n",
    "                # Using the median of norms worked better than the respective norm \n",
    "                scores[d] += (tf_td / config.norm)  * w_tq \n",
    "    for i in range(len(doc_ids)):\n",
    "        scores[i] /= Lengths[i]\n",
    "    \n",
    "    return scores \n",
    "\n",
    "def compute_relevance_feedback_scores(inverted_index, query, relevant_docs, idf, doc_ids, norms, Lengths):\n",
    "    scores = np.zeros(len(doc_ids))\n",
    "    tf_query = Counter(query)\n",
    "    \n",
    "    for t in tf_query.keys():\n",
    "        if t in inverted_index:\n",
    "            w_tq = ( tf_query[t] ) * (idf[t] ** 2 )\n",
    "            postings_list = inverted_index[t]\n",
    "            for (d, tf_td) in postings_list:\n",
    "                # Using the median of norms worked better than the respective norm \n",
    "                scores[d] += (tf_td / config.norm)  * w_tq \n",
    "    for document in relevant_docs:\n",
    "        tf_doc = Counter(document)\n",
    "        for t in tf_doc.keys():\n",
    "            if t in inverted_index:\n",
    "                w_tq = ( tf_doc[t] ) * (idf[t] ** 2 )\n",
    "                postings_list = inverted_index[t]\n",
    "                for (d, tf_td) in postings_list:\n",
    "                    # Using the median of norms worked better than the respective norm \n",
    "                    scores[d] += (tf_td / config.norm)  * w_tq \n",
    "                    \n",
    "    for i in range(len(doc_ids)):\n",
    "        scores[i] /= Lengths[i]\n",
    "    \n",
    "    return scores \n",
    "\n",
    "def compute_discounted_relevance_feedback_scores(inverted_index, query, relevant_docs, idf, doc_ids, norms, Lengths):\n",
    "    scores = np.zeros(len(doc_ids))\n",
    "    tf_query = Counter(query)\n",
    "    for t in tf_query.keys():\n",
    "        if t in inverted_index:\n",
    "            w_tq = ( tf_query[t] ) * (idf[t] ** 2 )\n",
    "            postings_list = inverted_index[t]\n",
    "            for (d, tf_td) in postings_list:\n",
    "                # Using the median of norms worked better than the respective norm \n",
    "                scores[d] += config.alpha*(tf_td / config.norm)  * w_tq \n",
    "    for enum, document in enumerate(relevant_docs):\n",
    "        tf_doc = Counter(document)\n",
    "        for t in tf_doc.keys():\n",
    "            if t in inverted_index:\n",
    "                w_tq = ( tf_doc[t] ) * (idf[t] ** 2 )\n",
    "                postings_list = inverted_index[t]\n",
    "                for (d, tf_td) in postings_list:\n",
    "                    # Discount factor scaled by rank of document in question \n",
    "                    scores[d] += ((1 - config.alpha)**(enum + 1))*(tf_td / config.norm)  * w_tq \n",
    "                    \n",
    "    for i in range(len(doc_ids)):\n",
    "        scores[i] /= Lengths[i]\n",
    "    \n",
    "    return scores \n",
    "\n",
    "def compute_tf_scores(inverted_index, query, doc_ids, Lengths):\n",
    "    scores = np.zeros(len(doc_ids))\n",
    "    tf_query = Counter(query)\n",
    "    for t in tf_query.keys():\n",
    "        if t in inverted_index:\n",
    "            w_tq = tf_query[t] * 1. # No IDF multiplication \n",
    "            postings_list = inverted_index[t]\n",
    "            for (d, tf_td) in postings_list:\n",
    "                scores[d] += tf_td * w_tq \n",
    "    for i in range(len(doc_ids)):\n",
    "        scores[i] /= Lengths[i]\n",
    "    \n",
    "    return scores \n",
    "\n",
    "def compute_boolean_scores(inverted_index, query, doc_ids): \n",
    "    scores = np.zeros(len(doc_ids))\n",
    "    query_norm = 1 / len(query)\n",
    "    for t in query:\n",
    "        if t in inverted_index:\n",
    "            postings_list = inverted_index[t]\n",
    "            for (d, _) in postings_list:\n",
    "                scores[d] += d * query_norm \n",
    "\n",
    "    # Tolerance of 0.75, If a document has more than 75% of the non stop-words terms in the query, it gets a score of 1 \n",
    "    scores = scores[ scores > 0.75]\n",
    "    scores = [int(s) for s in scores]\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_top_k_documents(doc_ids, scores, k):\n",
    "    idx = (-scores).argsort()[:k]\n",
    "    top_k_docs = [(i, doc_ids[i], scores[i]) for i in idx]\n",
    "    return top_k_docs\n",
    "\n",
    "def generate_log_string(top_k_docs, query_id):\n",
    "    log_string = ''\n",
    "    for rank, (relative_index, doc_id, score)  in enumerate(top_k_docs):\n",
    "        log_string += f'{query_id} 0 {doc_id} {rank+1} {score} tf-idf\\n'\n",
    "    return log_string\n",
    "\n",
    "def retrieve_documents(method, corpus, query, inverted_index, idf, doc_ids, norms, Lengths):  \n",
    "    if method == 'tf-idf':\n",
    "        scores = compute_tf_idf_scores(inverted_index, query, idf, doc_ids, norms, Lengths)\n",
    "        top_k_docs = get_top_k_documents(doc_ids, scores, 50)        \n",
    "\n",
    "    elif method =='tf':\n",
    "        scores = compute_tf_scores(inverted_index, query, doc_ids, Lengths)\n",
    "        top_k_docs = get_top_k_documents(doc_ids, scores, 50)\n",
    "    \n",
    "    elif method =='boolean':\n",
    "        scores = compute_boolean_scores(inverted_index, query, doc_ids)\n",
    "        top_k_docs = get_top_k_documents(doc_ids, scores, 50)\n",
    "\n",
    "    elif method == 'relevance-feedback':\n",
    "        scores = compute_tf_idf_scores(inverted_index, query, idf, doc_ids, norms, Lengths)\n",
    "        relevant_docs = get_top_k_documents(doc_ids, scores, 5)\n",
    "        stopwords_list = read_stopwords_file('stopwords.txt')\n",
    "        relevant_docs_processed = []\n",
    "        for (relative_index, document_id, score) in relevant_docs:\n",
    "            extra_document = corpus[relative_index]\n",
    "            # Clean document \n",
    "            extra_document = extra_document.split(\" \")\n",
    "\n",
    "            # Remove stopwords from document \n",
    "            extra_document = remove_stopwords(extra_document, stopwords_list)\n",
    "            extra_document = [token.lower() for token in extra_document]\n",
    "            \n",
    "            extra_document = ' '.join(extra_document)\n",
    "        \n",
    "            # Clean document \n",
    "            extra_document = clean_entry(extra_document)\n",
    "\n",
    "            # Tokenize document \n",
    "            tokens = extra_document.split(\" \")\n",
    "\n",
    "            relevant_docs_processed.append(tokens)\n",
    "\n",
    "        scores = compute_relevance_feedback_scores(inverted_index, query, relevant_docs_processed, idf, doc_ids, norms, Lengths)\n",
    "        top_k_docs = get_top_k_documents(doc_ids, scores, 50)        \n",
    "    \n",
    "    elif method == 'discounted-relevance-feedback':\n",
    "        scores = compute_tf_idf_scores(inverted_index, query, idf, doc_ids, norms, Lengths)\n",
    "        relevant_docs = get_top_k_documents(doc_ids, scores, 5)\n",
    "        stopwords_list = read_stopwords_file('stopwords.txt')\n",
    "        relevant_docs_processed = []\n",
    "        for (relative_index, document_id, score) in relevant_docs:\n",
    "            extra_document = corpus[relative_index]\n",
    "            # Clean document \n",
    "            extra_document = extra_document.split(\" \")\n",
    "\n",
    "            # Remove stopwords from document \n",
    "            extra_document = remove_stopwords(extra_document, stopwords_list)\n",
    "            extra_document = [token.lower() for token in extra_document]\n",
    "            \n",
    "            extra_document = ' '.join(extra_document)\n",
    "        \n",
    "            # Clean document \n",
    "            extra_document = clean_entry(extra_document)\n",
    "\n",
    "            # Tokenize document \n",
    "            tokens = extra_document.split(\" \")\n",
    "\n",
    "            relevant_docs_processed.append(tokens)\n",
    "\n",
    "        scores = compute_discounted_relevance_feedback_scores(inverted_index, query, relevant_docs_processed, idf, doc_ids, norms, Lengths)\n",
    "        top_k_docs = get_top_k_documents(doc_ids, scores, 50)    \n",
    "\n",
    "    return top_k_docs \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = split_into_documents(CORPUS_PATH)\n",
    "doc_ids = collect_document_ids(corpus)\n",
    "\n",
    "with open(QUERY_PATH) as f:\n",
    "    query_string = f.read()\n",
    "queries = re.split('</top>\\n', query_string)[:-1]\n",
    "\n",
    "# corpus = corpus[:500]\n",
    "# docs_ids = doc_ids[:500]\n",
    "\n",
    "inverted_index, Lengths = create_inverted_index(corpus, doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = get_document_norm(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271.9669097456896"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = calculate_idf(inverted_index, TOTAL_NUMBER_OF_DOCUMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_string = ''\n",
    "for query in queries:\n",
    "    query, query_id = process_query(query)\n",
    "    top_k_docs = retrieve_documents('tf-idf', corpus, query, inverted_index, idf, doc_ids, norms, Lengths)\n",
    "    log_string += generate_log_string(top_k_docs, query_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/saaket/Documents/Information Retrieval/search_engine/src/all.ipynb Cell 13'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saaket/Documents/Information%20Retrieval/search_engine/src/all.ipynb#ch0000013?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m queries:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saaket/Documents/Information%20Retrieval/search_engine/src/all.ipynb#ch0000013?line=2'>3</a>\u001b[0m     query, query_id \u001b[39m=\u001b[39m process_query(query)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/saaket/Documents/Information%20Retrieval/search_engine/src/all.ipynb#ch0000013?line=3'>4</a>\u001b[0m     top_k_docs \u001b[39m=\u001b[39m retrieve_documents(\u001b[39m'\u001b[39;49m\u001b[39mrelevance-feedback\u001b[39;49m\u001b[39m'\u001b[39;49m, corpus, query, inverted_index, idf, doc_ids, norms, Lengths)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saaket/Documents/Information%20Retrieval/search_engine/src/all.ipynb#ch0000013?line=4'>5</a>\u001b[0m     log_string \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m generate_log_string(top_k_docs, query_id)\n",
      "\u001b[1;32m/Users/saaket/Documents/Information Retrieval/search_engine/src/all.ipynb Cell 8'\u001b[0m in \u001b[0;36mretrieve_documents\u001b[0;34m(method, corpus, query, inverted_index, idf, doc_ids, norms, Lengths)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/saaket/Documents/Information%20Retrieval/search_engine/src/all.ipynb#ch0000001?line=145'>146</a>\u001b[0m         tokens \u001b[39m=\u001b[39m extra_document\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/saaket/Documents/Information%20Retrieval/search_engine/src/all.ipynb#ch0000001?line=147'>148</a>\u001b[0m         relevant_docs_processed\u001b[39m.\u001b[39mappend(tokens)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/saaket/Documents/Information%20Retrieval/search_engine/src/all.ipynb#ch0000001?line=149'>150</a>\u001b[0m     scores \u001b[39m=\u001b[39m compute_relevance_feedback_scores(inverted_index, query, relevant_docs_processed, idf, doc_ids, norms, Lengths)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/saaket/Documents/Information%20Retrieval/search_engine/src/all.ipynb#ch0000001?line=150'>151</a>\u001b[0m     top_k_docs \u001b[39m=\u001b[39m get_top_k_documents(doc_ids, scores, \u001b[39m50\u001b[39m)        \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/saaket/Documents/Information%20Retrieval/search_engine/src/all.ipynb#ch0000001?line=152'>153</a>\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdiscounted-relevance-feedback\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;32m/Users/saaket/Documents/Information Retrieval/search_engine/src/all.ipynb Cell 8'\u001b[0m in \u001b[0;36mcompute_relevance_feedback_scores\u001b[0;34m(inverted_index, query, relevant_docs, idf, doc_ids, norms, Lengths)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/saaket/Documents/Information%20Retrieval/search_engine/src/all.ipynb#ch0000001?line=36'>37</a>\u001b[0m             w_tq \u001b[39m=\u001b[39m ( tf_doc[t] ) \u001b[39m*\u001b[39m (idf[t] \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/saaket/Documents/Information%20Retrieval/search_engine/src/all.ipynb#ch0000001?line=37'>38</a>\u001b[0m             postings_list \u001b[39m=\u001b[39m inverted_index[t]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/saaket/Documents/Information%20Retrieval/search_engine/src/all.ipynb#ch0000001?line=38'>39</a>\u001b[0m             \u001b[39mfor\u001b[39;00m (d, tf_td) \u001b[39min\u001b[39;00m postings_list:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/saaket/Documents/Information%20Retrieval/search_engine/src/all.ipynb#ch0000001?line=39'>40</a>\u001b[0m                 \u001b[39m# Using the median of norms worked better than the respective norm \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/saaket/Documents/Information%20Retrieval/search_engine/src/all.ipynb#ch0000001?line=40'>41</a>\u001b[0m                 scores[d] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (tf_td \u001b[39m/\u001b[39m config\u001b[39m.\u001b[39mnorm)  \u001b[39m*\u001b[39m w_tq \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/saaket/Documents/Information%20Retrieval/search_engine/src/all.ipynb#ch0000001?line=42'>43</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(doc_ids)):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log_string = ''\n",
    "for query in queries:\n",
    "    query, query_id = process_query(query)\n",
    "    top_k_docs = retrieve_documents('relevance-feedback', corpus, query, inverted_index, idf, doc_ids, norms, Lengths)\n",
    "    log_string += generate_log_string(top_k_docs, query_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../logs/relevance_feedback_log_file.txt', 'w') as f:\n",
    "    f.write(log_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_string = ''\n",
    "for query in queries:\n",
    "    query, query_id = process_query(query)\n",
    "    top_k_docs = retrieve_documents('discounted-relevance-feedback', corpus, query, inverted_index, idf, doc_ids, norms, Lengths)\n",
    "    log_string += generate_log_string(top_k_docs, query_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../logs/discounted_relevance_feedback_log_file_alpha_7.txt', 'w') as f:\n",
    "    f.write(log_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python graphled",
   "language": "python",
   "name": "graphled"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
